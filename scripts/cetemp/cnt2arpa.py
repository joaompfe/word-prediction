"""
Converts a file of ngram counts generated by SRILM ngram-count command (see
http://www.speech.sri.com/projects/srilm/manpages/ngram-count.1.html) into an ARPA file in accordance with Stupid
Backoff model (which uses raw maximum likelihood probabilities with a backoff factor of 0.4).
"""
import math
import os
import sys
import uuid
from subprocess import run

from tqdm import tqdm


if __name__ == '__main__':
    in_path = sys.argv[1]
    order = int(sys.argv[2])
    out_path = sys.argv[3]
    backoff = None
    try:
        backoff = float(sys.argv[4])
    except IndexError:
        backoff = 0.4
    tmp_dir = None
    try:
        tmp_dir = sys.argv[5]
    except IndexError:
        tmp_dir = str(uuid.uuid4())

    if not os.path.exists(tmp_dir):
        os.makedirs(tmp_dir)

    tmp_files = [open(tmp_dir + "/" + str(i) + ".cnt", "w+") for i in range(order)]
    for i in range(order):
        tmp_files[i].write("\n\\%d-grams:\n" % (i + 1))

    error_file = open(tmp_dir + "/error.cnt", "w+")


    def read_next_ngram(f, order):
        prev_tokens = None
        tokens = f.readline().split()
        while len(tokens) != order + 1:
            prev_tokens = tokens
            tokens = f.readline().split()
        return tokens, prev_tokens


    def save_ngram(ngram, n, prob):
        line = str(prob) + " " + " ".join(ngram) + "\t" + str(backoff) + "\n"
        if (n - 1) >= order:
            error_file.write(line)
        else:
            tmp_files[n - 1].write(line)


    def same_context(ngram0, ngram1):
        return ngram0[0:len(ngram0) - 1] == ngram1[0:len(ngram1) - 1]


    total = 0
    with open(in_path) as f:
        for line in tqdm(f, desc="Calculating total of 1-grams"):
            tokens = line.split()
            if len(tokens) != 2:
                continue
            total += int(tokens[-1])

    lengths = [0] * order
    ngrams = [None] * order
    counts = [0] * order
    probs = [0] * order
    with open(in_path) as f:
        for line in tqdm(f, desc="Calculating n-gram probabilities"):
            tokens = line.split()
            ngram = tokens[0:len(tokens) - 1]
            n = len(ngram)
            count = float(tokens[-1])
            ctx = ngram[0:len(ngram)-1]

            if n != 1 and ctx != ngrams[n - 2]:
                error_file.write(line)
                continue

            if n == 1:
                prob = math.log10(count / total)
                save_ngram(ngram, n, prob)
            if n > 1:
                prob = math.log10(count / counts[n - 2]) + probs[n - 2]
                save_ngram(ngram, n, prob)
                if (n - 1) >= order:
                    continue
            ngrams[n - 1] = ngram
            counts[n - 1] = count
            lengths[n - 1] += 1
            probs[n - 1] = prob

    for file in tmp_files:
        file.close()

    error_file.close()

    with open(tmp_dir + "/header.cnt", "w+") as f:
        f.write("\\data\\\n")
        for i in range(order):
            f.write("ngram %d=%d\n" % (i + 1, lengths[i]))

    run("cat %s >> %s" % (tmp_dir + "/header.cnt", out_path), shell=True)
    for file in tmp_files:
        run("cat %s >> %s" % (file.name, out_path), shell=True)
